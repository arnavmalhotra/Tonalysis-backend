<!DOCTYPE html>
<html>
<head>
    <title>Live Speech Recognition Client</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }
        .main-container { display: flex; gap: 20px; }
        .left-panel { flex: 1; }
        .right-panel { flex: 1; }
        
        #messages { border: 1px solid #ccc; height: 200px; overflow-y: scroll; padding: 10px; margin-bottom: 10px; }
        button { padding: 10px 20px; margin: 5px; font-size: 16px; }
        .status { margin-bottom: 10px; font-weight: bold; }
        .transcription { color: green; }
        .partial { color: #666; font-style: italic; }
        .system { color: gray; font-style: italic; }
        .error { color: red; }
        .sent { color: blue; }
        #recordButton { background-color: #4CAF50; color: white; }
        #recordButton.recording { background-color: #f44336; }
        #currentTranscription { 
            padding: 15px; 
            background: #f0f0f0; 
            margin: 10px 0; 
            min-height: 60px; 
            border-radius: 5px;
            font-size: 18px;
        }
        .highlight { background-color: yellow; }
        #analysisSection {
            margin: 20px 0;
            padding: 15px;
            background: #e8f5e9;
            border-radius: 8px;
            border: 1px solid #4caf50;
        }
        #analysisSection h3 {
            margin-top: 0;
            color: #2e7d32;
        }
        .analysis-item {
            margin: 10px 0;
            padding: 10px;
            background: white;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .analysis-time {
            font-size: 12px;
            color: #666;
        }
        #timer {
            font-weight: bold;
            color: #1976d2;
        }
        
        /* Body Language Styles */
        #videoSection {
            margin: 20px 0;
            padding: 15px;
            background: #e3f2fd;
            border-radius: 8px;
            border: 1px solid #2196f3;
        }
        #videoSection h3 {
            margin-top: 0;
            color: #1565c0;
        }
        #cam {
            display: block;
            width: 100%;
            max-width: 640px;
            height: auto;
            border-radius: 8px;
            margin-bottom: 10px;
        }
        #overlay {
            border: 2px solid #2196f3;
            border-radius: 8px;
            max-width: 100%;
        }
        #bodyLanguageSection {
            margin: 20px 0;
            padding: 15px;
            background: #fff3e0;
            border-radius: 8px;
            border: 1px solid #ff9800;
        }
        #bodyLanguageSection h3 {
            margin-top: 0;
            color: #ef6c00;
        }
        .body-language-item {
            margin: 10px 0;
            padding: 10px;
            background: white;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .metric {
            display: inline-block;
            margin: 5px 10px 5px 0;
            padding: 5px 10px;
            background: #f5f5f5;
            border-radius: 15px;
            font-size: 12px;
        }
        .metric.good { background: #c8e6c9; color: #2e7d32; }
        .metric.warning { background: #ffecb3; color: #f57c00; }
        .metric.alert { background: #ffcdd2; color: #d32f2f; }
    </style>
</head>
<body>
    <h1>Live Speech & Body Language Therapy</h1>
    <div class="status" id="status">Disconnected</div>
    
    <div class="main-container">
        <div class="left-panel">
            <div id="currentTranscription">
                <strong>Live transcription:</strong> <span id="timer"></span>
                <div id="transcriptionText">Click "Start Recording" and speak...</div>
            </div>
            
            <div id="analysisSection">
                <h3>Speech Therapist Analysis</h3>
                <div id="analysisContent">Analysis will appear every 10 seconds while speaking...</div>
            </div>
            
            <div id="messages"></div>
            <button id="connectButton">Connect to Server</button>
            <button id="recordButton">Start Recording</button>
        </div>
        
        <div class="right-panel">
            <div id="videoSection">
                <h3>Video Analysis</h3>
                <div style="position: relative; display: inline-block;">
                    <video id="video" autoplay muted playsinline></video>
                    <canvas id="canvas" style="position: absolute; top: 0; left: 0; pointer-events: none;"></canvas>
                </div>
                <button id="cameraButton">Start Camera</button>
            </div>
            
            <div id="bodyLanguageSection">
                <h3>Body Language Analysis</h3>
                <div id="bodyLanguageContent">
                    <div class="body-language-item">
                        <strong>Current Status:</strong>
                        <div id="currentMetrics">
                            <span class="metric" id="emotionMetric">Emotion: Unknown</span>
                            <span class="metric" id="postureMetric">Posture: Unknown</span>
                            <span class="metric" id="fatigueMetric">Fatigue: Unknown</span>
                        </div>
                    </div>
                    <div id="bodyLanguageFeedback">Body language feedback will appear here...</div>
                </div>
            </div>
        </div>
    </div>

    <!-- State-of-the-art emotion detection -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script>
        console.log('MediaPipe Face Mesh loaded for accurate emotion detection');
    </script>
    <script src="body_language.js"></script>

    <script>
        let ws = null;
        let recognition = null;
        let isRecording = false;
        let analysisTimer = null;
        let secondsElapsed = 0;
        let isCameraActive = false;
        const clientId = Math.floor(Math.random() * 1000);
        const messages = document.getElementById('messages');
        const connectButton = document.getElementById('connectButton');
        const recordButton = document.getElementById('recordButton');
        const cameraButton = document.getElementById('cameraButton');
        const status = document.getElementById('status');
        const transcriptionText = document.getElementById('transcriptionText');
        const analysisContent = document.getElementById('analysisContent');
        const timer = document.getElementById('timer');
        const bodyLanguageContent = document.getElementById('bodyLanguageFeedback');
        const emotionMetric = document.getElementById('emotionMetric');
        const postureMetric = document.getElementById('postureMetric');
        const fatigueMetric = document.getElementById('fatigueMetric');

        // Check if browser supports speech recognition
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        
        if (!SpeechRecognition) {
            alert('Your browser does not support speech recognition. Please use Chrome, Edge, or Safari.');
            recordButton.disabled = true;
        }

        function addMessage(message, className) {
            const div = document.createElement('div');
            div.className = className;
            div.textContent = `${new Date().toLocaleTimeString()} - ${message}`;
            messages.appendChild(div);
            messages.scrollTop = messages.scrollHeight;
        }

        async function connect() {
            ws = new WebSocket(`ws://localhost:8000/ws/text/${clientId}`);

            ws.onopen = () => {
                status.textContent = `Connected as Client #${clientId}`;
                status.style.color = 'green';
                connectButton.textContent = 'Disconnect';
                addMessage('Connected to server', 'system');
            };

            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                if (data.type === 'response') {
                    // Don't show character count messages
                } else if (data.type === 'analysis') {
                    displayAnalysis(data);
                } else if (data.type === 'body_language_feedback') {
                    displayBodyLanguageFeedback(data);
                }
            };

            ws.onerror = (error) => {
                addMessage(`Error: ${error}`, 'error');
            };

            ws.onclose = () => {
                status.textContent = 'Disconnected';
                status.style.color = 'red';
                connectButton.textContent = 'Connect to Server';
                addMessage('Disconnected from server', 'system');
                ws = null;
            };
        }

        function disconnect() {
            if (ws) {
                ws.close();
            }
        }

        function displayAnalysis(data) {
            const analysisDiv = document.createElement('div');
            analysisDiv.className = 'analysis-item';
            
            const headerDiv = document.createElement('div');
            headerDiv.style.display = 'flex';
            headerDiv.style.justifyContent = 'space-between';
            headerDiv.style.marginBottom = '8px';
            
            const timeDiv = document.createElement('div');
            timeDiv.className = 'analysis-time';
            timeDiv.textContent = new Date(data.timestamp).toLocaleTimeString();
            
            const numberDiv = document.createElement('div');
            numberDiv.style.color = '#1976d2';
            numberDiv.style.fontWeight = 'bold';
            numberDiv.textContent = `Analysis #${data.analysis_number || 1}`;
            
            headerDiv.appendChild(timeDiv);
            headerDiv.appendChild(numberDiv);
            
            const textDiv = document.createElement('div');
            textDiv.innerHTML = `<strong>Feedback:</strong> ${data.text}`;
            
            const transcriptDiv = document.createElement('div');
            transcriptDiv.style.fontSize = '12px';
            transcriptDiv.style.color = '#666';
            transcriptDiv.style.marginTop = '5px';
            const wordCount = data.transcript_analyzed.split(' ').length;
            transcriptDiv.innerHTML = `<em>Based on ${wordCount} words: "${data.transcript_analyzed.substring(0, 50)}..."</em>`;
            
            analysisDiv.appendChild(headerDiv);
            analysisDiv.appendChild(textDiv);
            analysisDiv.appendChild(transcriptDiv);
            
            // Keep history of analyses
            if (!analysisContent.querySelector('.analysis-item')) {
                analysisContent.innerHTML = '';
            }
            
            // Insert new analysis at the top
            analysisContent.insertBefore(analysisDiv, analysisContent.firstChild);
            
            // Keep only last 3 analyses
            const analyses = analysisContent.querySelectorAll('.analysis-item');
            if (analyses.length > 3) {
                analyses[analyses.length - 1].remove();
            }
            
            addMessage(`Received analysis #${data.analysis_number || 1} from speech therapist`, 'system');
        }

        function displayBodyLanguageFeedback(data) {
            const feedbackDiv = document.createElement('div');
            feedbackDiv.className = 'body-language-item';
            
            const headerDiv = document.createElement('div');
            headerDiv.style.display = 'flex';
            headerDiv.style.justifyContent = 'space-between';
            headerDiv.style.marginBottom = '8px';
            
            const timeDiv = document.createElement('div');
            timeDiv.className = 'analysis-time';
            timeDiv.textContent = new Date(data.timestamp).toLocaleTimeString();
            
            const numberDiv = document.createElement('div');
            numberDiv.style.color = '#ef6c00';
            numberDiv.style.fontWeight = 'bold';
            numberDiv.textContent = `Body Language #${data.analysis_number || 1}`;
            
            headerDiv.appendChild(timeDiv);
            headerDiv.appendChild(numberDiv);
            
            const textDiv = document.createElement('div');
            textDiv.innerHTML = `<strong>Feedback:</strong> ${data.text}`;
            
            feedbackDiv.appendChild(headerDiv);
            feedbackDiv.appendChild(textDiv);
            
            // Keep history of feedback
            if (!bodyLanguageContent.querySelector('.body-language-item')) {
                bodyLanguageContent.innerHTML = '';
            }
            
            // Insert new feedback at the top (after current metrics)
            const currentMetricsDiv = bodyLanguageContent.querySelector('.body-language-item');
            if (currentMetricsDiv && currentMetricsDiv.nextSibling) {
                bodyLanguageContent.insertBefore(feedbackDiv, currentMetricsDiv.nextSibling);
            } else {
                bodyLanguageContent.appendChild(feedbackDiv);
            }
            
            // Keep only last 2 feedback items
            const feedbacks = bodyLanguageContent.querySelectorAll('.body-language-item');
            if (feedbacks.length > 3) { // Current metrics + 2 feedback items
                feedbacks[feedbacks.length - 1].remove();
            }
            
            addMessage(`Received body language analysis #${data.analysis_number || 1}`, 'system');
        }

        function updateBodyLanguageMetrics(emotion, posture, fatigue) {
            // Update emotion metric
            emotionMetric.textContent = `Emotion: ${emotion}`;
            emotionMetric.className = 'metric';
            if (['happy', 'surprised'].includes(emotion)) {
                emotionMetric.classList.add('good');
            } else if (['sad', 'angry', 'fearful'].includes(emotion)) {
                emotionMetric.classList.add('alert');
            } else {
                emotionMetric.classList.add('warning');
            }

            // Update posture metric
            postureMetric.textContent = `Posture: ${posture.label}`;
            postureMetric.className = 'metric';
            if (posture.label === 'good') {
                postureMetric.classList.add('good');
            } else if (posture.label === 'poor') {
                postureMetric.classList.add('alert');
            } else {
                postureMetric.classList.add('warning');
            }

            // Update fatigue metric
            fatigueMetric.textContent = `Energy: ${fatigue.label}`;
            fatigueMetric.className = 'metric';
            if (fatigue.label === 'alert') {
                fatigueMetric.classList.add('good');
            } else if (fatigue.label === 'tired') {
                fatigueMetric.classList.add('alert');
            } else {
                fatigueMetric.classList.add('warning');
            }
        }

        function updateTimer() {
            const nextAnalysis = 10 - (secondsElapsed % 10);
            timer.textContent = `(Next analysis in ${nextAnalysis}s)`;
        }

        function startRecording() {
            if (!SpeechRecognition) return;

            recognition = new SpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';

            let finalTranscript = '';
            let lastSentIndex = 0;

            recognition.onstart = () => {
                isRecording = true;
                recordButton.textContent = 'Stop Recording';
                recordButton.classList.add('recording');
                transcriptionText.textContent = 'Listening...';
                addMessage('Speech recognition started', 'system');
                
                // Start timer
                secondsElapsed = 0;
                analysisTimer = setInterval(() => {
                    secondsElapsed++;
                    updateTimer();
                }, 1000);
                updateTimer();
            };

            recognition.onresult = (event) => {
                let interimTranscript = '';
                let currentTranscript = '';
                
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript + ' ';
                        currentTranscript = finalTranscript;
                    } else {
                        interimTranscript += transcript;
                        currentTranscript = finalTranscript + interimTranscript;
                    }
                }
                
                // Stream ALL text (final + interim) to backend continuously
                if (ws && ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({
                        type: 'streaming_transcription',
                        text: currentTranscript.trim(),
                        is_final: interimTranscript === '',
                        client_id: clientId,
                        timestamp: new Date().toISOString()
                    }));
                }
                
                // Update the display with both final and interim results
                transcriptionText.innerHTML = 
                    finalTranscript + 
                    '<span class="highlight">' + interimTranscript + '</span>';
            };

            recognition.onerror = (event) => {
                addMessage(`Recognition error: ${event.error}`, 'error');
                if (event.error === 'no-speech') {
                    transcriptionText.textContent = 'No speech detected. Please try again.';
                }
            };

            recognition.onend = () => {
                isRecording = false;
                recordButton.textContent = 'Start Recording';
                recordButton.classList.remove('recording');
                addMessage('Speech recognition stopped', 'system');
                
                // Stop timer
                if (analysisTimer) {
                    clearInterval(analysisTimer);
                    analysisTimer = null;
                }
                timer.textContent = '';
                
                // Send any remaining text
                const remainingText = finalTranscript.substring(lastSentIndex);
                if (remainingText.trim() && ws && ws.readyState === WebSocket.OPEN) {
                    ws.send(JSON.stringify({
                        type: 'final_transcription',
                        text: remainingText.trim(),
                        client_id: clientId,
                        timestamp: new Date().toISOString()
                    }));
                }
            };

            recognition.start();
        }

        function stopRecording() {
            if (recognition && isRecording) {
                recognition.stop();
            }
        }

        connectButton.onclick = () => {
            if (ws) {
                disconnect();
            } else {
                connect();
            }
        };

        recordButton.onclick = () => {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        };

        cameraButton.onclick = async () => {
            if (isCameraActive) {
                // Stop camera
                const camElement = document.getElementById('cam');
                const stream = camElement ? camElement.srcObject : null;
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                    camElement.srcObject = null;
                }
                isCameraActive = false;
                cameraButton.textContent = 'Start Camera';
                addMessage('Camera stopped', 'system');
            } else {
                // Start camera and body language analysis
                addMessage('Starting camera and body language analysis...', 'system');
                try {
                    const success = await window.bodyLanguage.start();
                    if (success) {
                        // Set WebSocket reference for body language module
                        if (ws) {
                            window.bodyLanguage.setWebSocket(ws);
                        }
                        isCameraActive = true;
                        cameraButton.textContent = 'Stop Camera';
                        addMessage('Camera and body language analysis started', 'system');
                    } else {
                        addMessage('Failed to start camera or load ML models', 'error');
                    }
                } catch (error) {
                    addMessage(`Camera error: ${error.message}`, 'error');
                }
            }
        };

        // Listen for body language data updates to update UI
        window.addEventListener('bodyLanguageUpdate', (event) => {
            const { emotion, posture, fatigue } = event.detail;
            updateBodyLanguageMetrics(emotion, posture, fatigue);
        });

        // Enable camera button immediately since we use simple detection
        window.addEventListener('load', () => {
            cameraButton.disabled = false;
            addMessage('Simple face detection ready - camera enabled', 'system');
        });
    </script>
</body>
</html>